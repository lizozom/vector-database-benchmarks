{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Explore Converted Data\n",
    "\n",
    "Inspect the pre-processed Wikipedia chunks with embeddings in `data/converted/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to converted data\n",
    "data_dir = Path('../data/converted')\n",
    "batch_files = sorted(data_dir.glob('elasticsearch_batch_*.jsonl'))\n",
    "\n",
    "print(f\"Found {len(batch_files)} batch files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first batch and inspect structure\n",
    "with open(batch_files[0], 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"Lines in first batch: {len(lines)}\")\n",
    "print(f\"\\nFormat: alternating index/document lines (bulk format)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a sample document (every other line is the actual document)\n",
    "sample_doc = json.loads(lines[1])  # Second line is first document\n",
    "\n",
    "print(\"Document fields:\")\n",
    "for key in sample_doc.keys():\n",
    "    if key == 'embedding':\n",
    "        print(f\"  {key}: [{len(sample_doc[key])} dimensions]\")\n",
    "    else:\n",
    "        print(f\"  {key}: {repr(sample_doc[key])[:80]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample document content\n",
    "print(f\"Title: {sample_doc['title']}\")\n",
    "print(f\"Chunk index: {sample_doc['chunk_index']}\")\n",
    "print(f\"Text length: {sample_doc['text_length']}\")\n",
    "print(f\"\\nText preview:\\n{sample_doc['text'][:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding stats\n",
    "embedding = np.array(sample_doc['embedding'])\n",
    "\n",
    "print(f\"Embedding dimension: {len(embedding)}\")\n",
    "print(f\"\\nVector stats:\")\n",
    "print(f\"  Min: {embedding.min():.4f}\")\n",
    "print(f\"  Max: {embedding.max():.4f}\")\n",
    "print(f\"  Mean: {embedding.mean():.4f}\")\n",
    "print(f\"  Norm: {np.linalg.norm(embedding):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total documents across all batches\n",
    "total_docs = 0\n",
    "for f in batch_files:\n",
    "    with open(f, 'r') as file:\n",
    "        # Each doc has 2 lines (index action + document)\n",
    "        total_docs += sum(1 for _ in file) // 2\n",
    "\n",
    "print(f\"Total documents: {total_docs:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total storage size\n",
    "total_size = sum(f.stat().st_size for f in batch_files)\n",
    "print(f\"Total storage: {total_size / (1024**3):.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "database-reviews-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
