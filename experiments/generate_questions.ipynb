{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Generate Evaluation Questions\n",
    "\n",
    "Generate questions from random Wikipedia article titles using Cohere LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import yaml\n",
    "import cohere\n",
    "from elasticsearch import Elasticsearch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ELASTICSEARCH_ENDPOINT = os.getenv(\"ELASTICSEARCH_ENDPOINT\")\n",
    "ELASTICSEARCH_API_KEY = os.getenv(\"ELASTICSEARCH_API_KEY\")\n",
    "INDEX_NAME = os.getenv(\"INDEX_NAME\").lower()\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "es_client = Elasticsearch(\n",
    "    ELASTICSEARCH_ENDPOINT,\n",
    "    api_key=ELASTICSEARCH_API_KEY\n",
    ")\n",
    "\n",
    "co = cohere.Client(COHERE_API_KEY)\n",
    "\n",
    "print(f\"Connected to Elasticsearch: {es_client.info()['version']['number']}\")\n",
    "print(f\"Index: {INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_titles(n: int = 1000) -> list[str]:\n",
    "    \"\"\"Retrieve n random unique article titles from the index.\"\"\"\n",
    "    \n",
    "    # Use random_score to get random documents, then dedupe titles\n",
    "    response = es_client.search(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            \"size\": n * 3,  # Oversample to get enough unique titles\n",
    "            \"query\": {\n",
    "                \"function_score\": {\n",
    "                    \"query\": {\"match_all\": {}},\n",
    "                    \"random_score\": {\"seed\": random.randint(1, 100000), \"field\": \"_seq_no\"}\n",
    "                }\n",
    "            },\n",
    "            \"_source\": [\"title\"]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Extract unique titles\n",
    "    titles = list(set(hit[\"_source\"][\"title\"] for hit in response[\"hits\"][\"hits\"]))\n",
    "    random.shuffle(titles)\n",
    "    \n",
    "    return titles[:n]\n",
    "\n",
    "titles = get_random_titles(1000)\n",
    "print(f\"Retrieved {len(titles)} unique titles\")\n",
    "print(f\"\\nSample titles:\")\n",
    "for t in titles[:10]:\n",
    "    print(f\"  - {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"You are given a list of Wikipedia article titles. For each title, generate exactly 3 questions that someone might ask to find information from that article.\n",
    "\n",
    "Guidelines:\n",
    "- Questions should be specific and factual (e.g., \"Where was X born?\", \"When did X start?\", \"What is the population of X?\")\n",
    "- Questions can also be broad but concrete (e.g., \"List popular football players from the 90s\", \"What are the main exports of X?\")\n",
    "- Do NOT use vague questions like \"What's so good about X?\" or \"Why is X important?\"\n",
    "- Questions should be answerable from a Wikipedia article\n",
    "\n",
    "Return ONLY a YAML list in this exact format (no other text):\n",
    "\n",
    "- title: Article Title 1\n",
    "  question: First question about Article 1?\n",
    "- title: Article Title 1\n",
    "  question: Second question about Article 1?\n",
    "- title: Article Title 1\n",
    "  question: Third question about Article 1?\n",
    "- title: Article Title 2\n",
    "  question: First question about Article 2?\n",
    "...\n",
    "\n",
    "Article titles:\n",
    "{titles}\n",
    "\"\"\"\n",
    "\n",
    "def generate_questions_batch(titles_batch: list[str]) -> list[dict]:\n",
    "    \"\"\"Generate questions for a batch of titles using Cohere.\"\"\"\n",
    "    \n",
    "    titles_str = \"\\n\".join(f\"- {t}\" for t in titles_batch)\n",
    "    prompt = PROMPT_TEMPLATE.format(titles=titles_str)\n",
    "    \n",
    "    response = co.chat(\n",
    "        model=\"command-r-plus\",\n",
    "        message=prompt,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    # Parse YAML response\n",
    "    response_text = response.text.strip()\n",
    "    \n",
    "    # Remove markdown code blocks if present\n",
    "    if response_text.startswith(\"```\"):\n",
    "        response_text = response_text.split(\"```\")[1]\n",
    "        if response_text.startswith(\"yaml\"):\n",
    "            response_text = response_text[4:]\n",
    "    \n",
    "    try:\n",
    "        questions = yaml.safe_load(response_text)\n",
    "        return questions if questions else []\n",
    "    except yaml.YAMLError as e:\n",
    "        print(f\"YAML parse error: {e}\")\n",
    "        print(f\"Response: {response_text[:500]}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a small batch\n",
    "test_batch = titles[:3]\n",
    "print(f\"Testing with: {test_batch}\")\n",
    "test_questions = generate_questions_batch(test_batch)\n",
    "print(f\"\\nGenerated {len(test_questions)} questions:\")\n",
    "for q in test_questions:\n",
    "    print(f\"  [{q['title']}] {q['question']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate questions for all titles in batches of 10\n",
    "BATCH_SIZE = 10\n",
    "all_questions = []\n",
    "\n",
    "for i in range(0, len(titles), BATCH_SIZE):\n",
    "    batch = titles[i:i + BATCH_SIZE]\n",
    "    batch_num = i // BATCH_SIZE + 1\n",
    "    total_batches = (len(titles) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    \n",
    "    print(f\"Processing batch {batch_num}/{total_batches}...\")\n",
    "    \n",
    "    questions = generate_questions_batch(batch)\n",
    "    all_questions.extend(questions)\n",
    "    \n",
    "    print(f\"  Generated {len(questions)} questions (total: {len(all_questions)})\")\n",
    "\n",
    "print(f\"\\nTotal questions generated: {len(all_questions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save questions to YAML file\n",
    "output_path = \"../data/evaluation_questions.yaml\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    yaml.dump(all_questions, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(f\"Saved {len(all_questions)} questions to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview saved questions\n",
    "print(\"Sample questions:\")\n",
    "for q in all_questions[:15]:\n",
    "    print(f\"  [{q['title']}] {q['question']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "database-reviews-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
